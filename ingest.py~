import requests
import os
from PIL import Image
from colpali_engine.models import ColQwen2, ColQwen2Processor
from transformers.utils.import_utils import is_flash_attn_2_available
import torch
import torch.nn.functional as F
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, Dataset, DistributedSampler
from tqdm import tqdm
from pdf2image import convert_from_path
import numpy as np
import time
import subprocess

print(torch.cuda.device_count())

source_folder = "/scratch/project_462000824/amoisala/RAG-60K/data/copernicus"
model_name = "vidore/colqwen2-v1.0"
index_path = "./index/"
os.makedirs(index_path, exist_ok=True)


def convert_pdfs_to_images(pdf_folder):
    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(".pdf")][:100] # select first 100
    all_images = {}

    for doc_id, pdf_file in enumerate(pdf_files):
        pdf_path = os.path.join(pdf_folder, pdf_file)
        images = convert_from_path(pdf_path)
        all_images[doc_id] = images

    return all_images

all_images = convert_pdfs_to_images(source_folder)


def get_gpu_usage():
    result = subprocess.run(["rocm-smi", "--showuse", "--showmemuse"], capture_output=True, text=True)
    print(result.stdout)


class PDFImagesDataset(Dataset):
    def __init__(self, images_dict):
        self.pages = []  # list of (doc_id, page_index, PIL_image)
        for doc_id, pages in images_dict.items():
            for i, img in enumerate(pages):
                self.pages.append((doc_id, i, img))
    def __len__(self):
        return len(self.pages)
    def __getitem__(self, idx):
        doc_id, page_idx, img = self.pages[idx]
        return img # returns a list of PIL images to be divided evenly for GPUs

    
# Main function
def main():
    dist.init_process_group(backend='nccl')

    local_rank = int(os.environ['LOCAL_RANK'])
    rank = dist.get_rank()
    world_size = dist.get_world_size()

    torch.cuda.set_device(local_rank)
    device = torch.device(f"cuda:{local_rank}")

    # define the model and processor 
    model = ColQwen2.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    attn_implementation="flash_attention_2" if is_flash_attn_2_available() else None,
    ).to(device)
    
    # Make DDP happy by pretending the model has trainable parameters
    for p in model.parameters():                                                  
        p.requires_grad = True
        break  # only one param needs to be enabled   
    processor = ColQwen2Processor.from_pretrained(model_name)
    # use pytorch DDP, copy the model to each gpu id
    model = DDP(model, device_ids=[local_rank])
    model.eval()
    #this will distribute data across multiple GPUs. It ensures that each process (GPU) gets a unique subset of the dataset without overlap, preventing redundant computations
    dataset = PDFImagesDataset(all_images)
    sampler = DistributedSampler(dataset, shuffle=False)

    # Process the inputs by batches
    dataloader = DataLoader(
        dataset,
        batch_size=4,
        shuffle=False,
        sampler=sampler, # add sampler
        num_workers=7,
        pin_memory=True,
        collate_fn=lambda x: processor.process_images(x),
    )
        
    local_embeddings = []
    torch.cuda.synchronize()
    start_time = time.time()
    
    for batch_doc in tqdm(dataloader):
        with torch.no_grad():
            batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}
            embeddings_doc = model(**batch_doc)
            local_embeddings.extend(list(torch.unbind(embeddings_doc.to("cpu"))))

    torch.cuda.synchronize()                                             
    end_time = time.time()                                                       
    elapsed = end_time - start_time
    print(f"Total embedding time: {elapsed:.3f} seconds")
    
    get_gpu_usage()
    
    local_embeddings = torch.stack([emb.to(dtype=torch.float32, device='cpu') for emb in local_embeddings])
    
    print(f"this is rank {rank}, the shape of the local_embeddings is {local_embeddings.shape}")

    dist.barrier()  # Ensures all processes complete before shutdown
    torch.save(local_embeddings, f"{index_path}/data{rank}.pt")
    print("Index saved")
    dist.destroy_process_group()

if __name__ == "__main__":
    main()
